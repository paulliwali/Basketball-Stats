algorithm = "gbm",
grid_id = "depth_grid",
x = c("Pick", "Height", "Weight", "MP", "FG", "FGA", "3P", "3Pa", "FT", "FTA", "ORB", "TRB",
"AST", "STL", "BLK", "TOV", "PF", "PTS", "College Pace", "OffRtg", "DefRtg", "Years in College"),
y = "IsSuccessful",
training_frame = train,
# Using stopping criteria so set ntrees to high number
ntrees = 10000,
# Learn rate shrinks by 1% after every tree
learn_rate = 0.05,
learn_rate_annealing = 0.99,
sample_rate_per_class = c(0.25, 1),
# Stops after 5 consecutive rounds of not improving by 0.01% on the AUC
stopping_tolerance = 0.0001,
stopping_rounds = 5,
stopping_metric = 'AUC',
score_tree_interval = 10,
seed = 42
)
# Display the grid by decreasing AUC
sortGrid <- h2o.getGrid("depth_grid", sort_by = "auc", decreasing = TRUE)
# save model
gbm <- h2o.saveModel(object=model, path = "models/", force=TRUE)
# launch h2o
h2o.flow()
# Display the grid by decreasing AUC
sortGrid <- h2o.getGrid("depth_grid", sort_by = "auc", decreasing = TRUE)
View(sortGrid)
sortGrid
model <- h2o.gbm(
model_id = "initial_train",
x = c("Height", "Weight", "MP", "FG", "FGA", "3P", "3Pa", "FT", "FTA", "ORB", "TRB",
"AST", "STL", "BLK", "TOV", "PF", "PTS", "Years in College"),
y = "IsSuccessful",
training_frame = train,
ntrees = 10000,
learn_rate = 0.001,
learn_rate_annealing = 0.99,
stopping_tolerance = 0.0001,
stopping_rounds = 10,
stopping_metric = 'logloss',
# This specifies that each tree in the ensemble should sample without replacement
# from the full training dataset using a per class sampling rate. Good for imbalanced
# datasets
sample_rate_per_class = c(0.25, 1),
# sample_rate = 0.75,
# col_sample_rate = 0.95,
score_tree_interval = 10,
max_depth = 3,
min_rows = 10,
distribution = "AUTO",
# fold_assignment = "Stratified",
nfolds = 5,
# keep_cross_validation_predictions = TRUE,
seed = 42
)
h2o.auc(h2o.performance(model, newdata = test))
h2o.auc(h2o.performance(model, xval = TRUE))
model@model$cross_validation_metrics_summary
model <- h2o.gbm(
model_id = "initial_train",
x = c("Height", "Weight", "MP", "FG", "FGA", "3P", "3Pa", "FT", "FTA", "ORB", "TRB",
"AST", "STL", "BLK", "TOV", "PF", "PTS", "Years in College"),
y = "IsSuccessful",
training_frame = train,
ntrees = 10000,
learn_rate = 0.01,
learn_rate_annealing = 0.99,
stopping_tolerance = 0.0001,
stopping_rounds = 10,
stopping_metric = 'logloss',
# This specifies that each tree in the ensemble should sample without replacement
# from the full training dataset using a per class sampling rate. Good for imbalanced
# datasets
sample_rate_per_class = c(0.25, 1),
# sample_rate = 0.75,
# col_sample_rate = 0.95,
score_tree_interval = 10,
max_depth = 3,
min_rows = 10,
distribution = "AUTO",
# fold_assignment = "Stratified",
nfolds = 5,
# keep_cross_validation_predictions = TRUE,
seed = 42
)
model <- h2o.gbm(
model_id = "initial_train",
x = c("Height", "Weight", "MP", "FG", "FGA", "3P", "3Pa", "FT", "FTA", "ORB", "TRB",
"AST", "STL", "BLK", "TOV", "PF", "PTS", "Years in College"),
y = "IsSuccessful",
training_frame = train,
ntrees = 10000,
learn_rate = 0.001,
learn_rate_annealing = 0.95,
stopping_tolerance = 0.0001,
stopping_rounds = 10,
stopping_metric = 'logloss',
# This specifies that each tree in the ensemble should sample without replacement
# from the full training dataset using a per class sampling rate. Good for imbalanced
# datasets
sample_rate_per_class = c(0.25, 1),
# sample_rate = 0.75,
# col_sample_rate = 0.95,
score_tree_interval = 10,
max_depth = 3,
min_rows = 10,
distribution = "AUTO",
# fold_assignment = "Stratified",
nfolds = 5,
# keep_cross_validation_predictions = TRUE,
seed = 42
)
model <- h2o.gbm(
model_id = "initial_train",
x = c("Height", "Weight", "MP", "FG", "FGA", "3P", "3Pa", "FT", "FTA", "ORB", "TRB",
"AST", "STL", "BLK", "TOV", "PF", "PTS", "Years in College"),
y = "IsSuccessful",
training_frame = train,
ntrees = 10000,
learn_rate = 0.0001,
learn_rate_annealing = 0.98,
stopping_tolerance = 0.0001,
stopping_rounds = 10,
stopping_metric = 'logloss',
# This specifies that each tree in the ensemble should sample without replacement
# from the full training dataset using a per class sampling rate. Good for imbalanced
# datasets
sample_rate_per_class = c(0.25, 1),
# sample_rate = 0.75,
# col_sample_rate = 0.95,
score_tree_interval = 10,
max_depth = 3,
min_rows = 10,
distribution = "AUTO",
# fold_assignment = "Stratified",
nfolds = 5,
# keep_cross_validation_predictions = TRUE,
seed = 42
)
model <- h2o.gbm(
model_id = "initial_train",
x = c("Height", "Weight", "MP", "FG", "FGA", "3P", "3Pa", "FT", "FTA", "ORB", "TRB",
"AST", "STL", "BLK", "TOV", "PF", "PTS", "Years in College"),
y = "IsSuccessful",
training_frame = train,
ntrees = 10000,
learn_rate = 0.0001,
learn_rate_annealing = 0.98,
stopping_tolerance = 0.0001,
stopping_rounds = 10,
stopping_metric = 'logloss',
# This specifies that each tree in the ensemble should sample without replacement
# from the full training dataset using a per class sampling rate. Good for imbalanced
# datasets
sample_rate_per_class = c(0.25, 1),
# sample_rate = 0.75,
# col_sample_rate = 0.95,
score_tree_interval = 10,
max_depth = 3,
min_rows = 5,
distribution = "AUTO",
# fold_assignment = "Stratified",
nfolds = 5,
# keep_cross_validation_predictions = TRUE,
seed = 42
)
model <- h2o.gbm(
model_id = "initial_train",
x = c("Height", "Weight", "MP", "FG", "FGA", "3P", "3Pa", "FT", "FTA", "ORB", "TRB",
"AST", "STL", "BLK", "TOV", "PF", "PTS", "Years in College"),
y = "IsSuccessful",
training_frame = train,
ntrees = 10000,
learn_rate = 0.0001,
learn_rate_annealing = 0.98,
stopping_tolerance = 0.0001,
stopping_rounds = 10,
stopping_metric = 'logloss',
# This specifies that each tree in the ensemble should sample without replacement
# from the full training dataset using a per class sampling rate. Good for imbalanced
# datasets
sample_rate_per_class = c(0.25, 1),
# sample_rate = 0.75,
# col_sample_rate = 0.95,
score_tree_interval = 10,
max_depth = 5,
min_rows = 5,
distribution = "AUTO",
# fold_assignment = "Stratified",
nfolds = 5,
# keep_cross_validation_predictions = TRUE,
seed = 42
)
model <- h2o.gbm(
model_id = "initial_train",
x = c("Height", "Weight", "MP", "FG", "FGA", "3P", "3Pa", "FT", "FTA", "ORB", "TRB",
"AST", "STL", "BLK", "TOV", "PF", "PTS", "Years in College"),
y = "IsSuccessful",
training_frame = train,
ntrees = 10000,
learn_rate = 0.0001,
learn_rate_annealing = 0.98,
stopping_tolerance = 0.0001,
stopping_rounds = 10,
stopping_metric = 'logloss',
# This specifies that each tree in the ensemble should sample without replacement
# from the full training dataset using a per class sampling rate. Good for imbalanced
# datasets
sample_rate_per_class = c(0.25, 1),
# sample_rate = 0.75,
# col_sample_rate = 0.95,
score_tree_interval = 10,
max_depth = 5,
min_rows = 10,
distribution = "AUTO",
# fold_assignment = "Stratified",
nfolds = 5,
# keep_cross_validation_predictions = TRUE,
seed = 42
)
model <- h2o.gbm(
model_id = "initial_train",
x = c("Height", "Weight", "MP", "FG", "FGA", "3P", "3Pa", "FT", "FTA", "ORB", "TRB",
"AST", "STL", "BLK", "TOV", "PF", "PTS", "Years in College"),
y = "IsSuccessful",
training_frame = train,
ntrees = 10000,
learn_rate = 0.0001,
learn_rate_annealing = 0.98,
stopping_tolerance = 0.0001,
stopping_rounds = 10,
stopping_metric = 'logloss',
# This specifies that each tree in the ensemble should sample without replacement
# from the full training dataset using a per class sampling rate. Good for imbalanced
# datasets
sample_rate_per_class = c(0.25, 1),
# sample_rate = 0.75,
# col_sample_rate = 0.95,
score_tree_interval = 10,
max_depth = 5,
min_rows = 10,
distribution = "AUTO",
# fold_assignment = "Stratified",
nfolds = 3,
# keep_cross_validation_predictions = TRUE,
seed = 42
)
library(tidyverse)
library(h2o)
library(rsample)
data <- read_csv("data/working/2000-2017-pace.csv", comment="#") %>%
mutate(IsSuccessful = as.factor(IsSuccessful)) %>%
mutate(`College Name` = as.factor(`College Name`)) %>%
mutate(FG.adj = FG/`College Pace`) %>%
mutate(FGA.adj = FGA/`College Pace`) %>%
mutate(`3P.adj` = `3P`/`College Pace`) %>%
mutate(`3PA.adj` = `3PA`/`College Pace`) %>%
mutate(FT.adj = FT/`College Pace`) %>%
mutate(FTA.adj = FTA/`College Pace`) %>%
mutate(ORB.adj = ORB/`College Pace`) %>%
mutate(TRB.adj = TRB/`College Pace`) %>%
mutate(AST.adj = AST/`College Pace`) %>%
mutate(STL.adj = STL/`College Pace`) %>%
mutate(BLK.adj = BLK/`College Pace`) %>%
mutate(TOV.adj = TOV/`College Pace`) %>%
mutate(PF.adj = PF/`College Pace`) %>%
mutate(PTS.adj = PTA/`College Pace`)
data <- read_csv("data/working/2000-2017-pace.csv", comment="#") %>%
mutate(IsSuccessful = as.factor(IsSuccessful)) %>%
mutate(`College Name` = as.factor(`College Name`)) %>%
mutate(FG.adj = FG/`College Pace`) %>%
mutate(FGA.adj = FGA/`College Pace`) %>%
mutate(`3P.adj` = `3P`/`College Pace`) %>%
mutate(`3Pa.adj` = `3Pa`/`College Pace`) %>%
mutate(FT.adj = FT/`College Pace`) %>%
mutate(FTA.adj = FTA/`College Pace`) %>%
mutate(ORB.adj = ORB/`College Pace`) %>%
mutate(TRB.adj = TRB/`College Pace`) %>%
mutate(AST.adj = AST/`College Pace`) %>%
mutate(STL.adj = STL/`College Pace`) %>%
mutate(BLK.adj = BLK/`College Pace`) %>%
mutate(TOV.adj = TOV/`College Pace`) %>%
mutate(PF.adj = PF/`College Pace`) %>%
mutate(PTS.adj = PTA/`College Pace`)
data <- read_csv("data/working/2000-2017-pace.csv", comment="#") %>%
mutate(IsSuccessful = as.factor(IsSuccessful)) %>%
mutate(`College Name` = as.factor(`College Name`)) %>%
mutate(FG.adj = FG/`College Pace`) %>%
mutate(FGA.adj = FGA/`College Pace`) %>%
mutate(`3P.adj` = `3P`/`College Pace`) %>%
mutate(`3Pa.adj` = `3Pa`/`College Pace`) %>%
mutate(FT.adj = FT/`College Pace`) %>%
mutate(FTA.adj = FTA/`College Pace`) %>%
mutate(ORB.adj = ORB/`College Pace`) %>%
mutate(TRB.adj = TRB/`College Pace`) %>%
mutate(AST.adj = AST/`College Pace`) %>%
mutate(STL.adj = STL/`College Pace`) %>%
mutate(BLK.adj = BLK/`College Pace`) %>%
mutate(TOV.adj = TOV/`College Pace`) %>%
mutate(PF.adj = PF/`College Pace`) %>%
mutate(PTS.adj = PTS/`College Pace`)
set.seed(42)
data_split <- initial_split(data, prop = 0.8,)
train_data <- training(data_split)
test_data <- testing(data_split)
table(train_data$IsSuccessful)
table(test_data$IsSuccessful)
# There is an imbalance of data, might consider upping the successful sample
h2o.show_progress()
h2o.init(nthreads = -1, max_mem_size = "12g")
train <- as.h2o(train_data)
test <- as.h2o(test_data)
model <- h2o.gbm(
model_id = "initial_train",
x = c("Height", "Weight", "MP", "FG", "FGA", "3P", "3Pa", "FT", "FTA", "ORB", "TRB",
"AST", "STL", "BLK", "TOV", "PF", "PTS", "Years in College"),
y = "IsSuccessful",
training_frame = train,
ntrees = 10000,
learn_rate = 0.0001,
learn_rate_annealing = 0.98,
stopping_tolerance = 0.0001,
stopping_rounds = 10,
stopping_metric = 'logloss',
# This specifies that each tree in the ensemble should sample without replacement
# from the full training dataset using a per class sampling rate. Good for imbalanced
# datasets
sample_rate_per_class = c(0.25, 1),
# sample_rate = 0.75,
# col_sample_rate = 0.95,
score_tree_interval = 10,
max_depth = 5,
min_rows = 10,
distribution = "AUTO",
# fold_assignment = "Stratified",
nfolds = 3,
# keep_cross_validation_predictions = TRUE,
seed = 42
)
h2o.auc(h2o.performance(model, newdata = test))
h2o.auc(h2o.performance(model, xval = TRUE))
model@model$cross_validation_metrics_summary
# save model
gbm <- h2o.saveModel(object=model, path = "models/", force=TRUE)
# launch h2o
h2o.flow()
model <- h2o.gbm(
model_id = "initial_train",
x = c("Height", "Weight", "MP", "FG", "FGA", "3P", "3Pa", "FT", "FTA", "ORB", "TRB",
"AST", "STL", "BLK", "TOV", "PF", "PTS", "Years in College",
"MP.adj", "FG.adj", "FGA.adj", "3P.adj", "3Pa.adj", "FT.adj", "FTA.adj", "ORB.adj", "TRB.adj",
"AST.adj", "STL.adj", "BLK.adj", "TOV.adj", "PF.adj", "PTS.adj"),
y = "IsSuccessful",
training_frame = train,
ntrees = 10000,
learn_rate = 0.0001,
learn_rate_annealing = 0.98,
stopping_tolerance = 0.0001,
stopping_rounds = 10,
stopping_metric = 'logloss',
# This specifies that each tree in the ensemble should sample without replacement
# from the full training dataset using a per class sampling rate. Good for imbalanced
# datasets
sample_rate_per_class = c(0.25, 1),
# sample_rate = 0.75,
# col_sample_rate = 0.95,
score_tree_interval = 10,
max_depth = 5,
min_rows = 10,
distribution = "AUTO",
# fold_assignment = "Stratified",
nfolds = 3,
# keep_cross_validation_predictions = TRUE,
seed = 42
)
model <- h2o.gbm(
model_id = "initial_train",
x = c("Height", "Weight", "MP", "FG", "FGA", "3P", "3Pa", "FT", "FTA", "ORB", "TRB",
"AST", "STL", "BLK", "TOV", "PF", "PTS", "Years in College",
"FG.adj", "FGA.adj", "3P.adj", "3Pa.adj", "FT.adj", "FTA.adj", "ORB.adj", "TRB.adj",
"AST.adj", "STL.adj", "BLK.adj", "TOV.adj", "PF.adj", "PTS.adj"),
y = "IsSuccessful",
training_frame = train,
ntrees = 10000,
learn_rate = 0.0001,
learn_rate_annealing = 0.98,
stopping_tolerance = 0.0001,
stopping_rounds = 10,
stopping_metric = 'logloss',
# This specifies that each tree in the ensemble should sample without replacement
# from the full training dataset using a per class sampling rate. Good for imbalanced
# datasets
sample_rate_per_class = c(0.25, 1),
# sample_rate = 0.75,
# col_sample_rate = 0.95,
score_tree_interval = 10,
max_depth = 5,
min_rows = 10,
distribution = "AUTO",
# fold_assignment = "Stratified",
nfolds = 3,
# keep_cross_validation_predictions = TRUE,
seed = 42
)
h2o.auc(h2o.performance(model, newdata = test))
h2o.auc(h2o.performance(model, xval = TRUE))
model@model$cross_validation_metrics_summary
# save model
gbm <- h2o.saveModel(object=model, path = "models/", force=TRUE)
test_data <- read_csv("data/working/2018-2018-pace.csv", comment="#")
test_data <- read_csv("data/working/2018-2018-pace-working.csv", comment="#") %>%
mutate(`College Name` = as.factor(`College Name`)) %>%
mutate(FG.adj = FG/`College Pace`) %>%
mutate(FGA.adj = FGA/`College Pace`) %>%
mutate(`3P.adj` = `3P`/`College Pace`) %>%
mutate(`3Pa.adj` = `3Pa`/`College Pace`) %>%
mutate(FT.adj = FT/`College Pace`) %>%
mutate(FTA.adj = FTA/`College Pace`) %>%
mutate(ORB.adj = ORB/`College Pace`) %>%
mutate(TRB.adj = TRB/`College Pace`) %>%
mutate(AST.adj = AST/`College Pace`) %>%
mutate(STL.adj = STL/`College Pace`) %>%
mutate(BLK.adj = BLK/`College Pace`) %>%
mutate(TOV.adj = TOV/`College Pace`) %>%
mutate(PF.adj = PF/`College Pace`) %>%
mutate(PTS.adj = PTS/`College Pace`)
View(test_data)
real_test <- as.h2o(test_data)
data_2018 <- read_csv("data/working/2018-2018-pace-working.csv", comment="#") %>%
mutate(`College Name` = as.factor(`College Name`)) %>%
mutate(FG.adj = FG/`College Pace`) %>%
mutate(FGA.adj = FGA/`College Pace`) %>%
mutate(`3P.adj` = `3P`/`College Pace`) %>%
mutate(`3Pa.adj` = `3Pa`/`College Pace`) %>%
mutate(FT.adj = FT/`College Pace`) %>%
mutate(FTA.adj = FTA/`College Pace`) %>%
mutate(ORB.adj = ORB/`College Pace`) %>%
mutate(TRB.adj = TRB/`College Pace`) %>%
mutate(AST.adj = AST/`College Pace`) %>%
mutate(STL.adj = STL/`College Pace`) %>%
mutate(BLK.adj = BLK/`College Pace`) %>%
mutate(TOV.adj = TOV/`College Pace`) %>%
mutate(PF.adj = PF/`College Pace`) %>%
mutate(PTS.adj = PTS/`College Pace`)
real_test <- as.h2o(data_2018)
View(real_test)
data_2018 <- read_csv("data/working/2018-2018-pace-working.csv", comment="#") %>%
mutate(`College Name` = as.factor(`College Name`)) %>%
mutate(FG.adj = FG/`College Pace`) %>%
mutate(FGA.adj = FGA/`College Pace`) %>%
mutate(`3P.adj` = `3P`/`College Pace`) %>%
mutate(`3Pa.adj` = `3Pa`/`College Pace`) %>%
mutate(FT.adj = FT/`College Pace`) %>%
mutate(FTA.adj = FTA/`College Pace`) %>%
mutate(ORB.adj = ORB/`College Pace`) %>%
mutate(TRB.adj = TRB/`College Pace`) %>%
mutate(AST.adj = AST/`College Pace`) %>%
mutate(STL.adj = STL/`College Pace`) %>%
mutate(BLK.adj = BLK/`College Pace`) %>%
mutate(TOV.adj = TOV/`College Pace`) %>%
mutate(PF.adj = PF/`College Pace`) %>%
mutate(PTS.adj = PTS/`College Pace`)
real_test <- as.h2o(data_2018)
View(real_test)
View(test)
h2o.predict(model, real_test)
h2o.predict(model, test)
prediction <- h2o.predict(model, test)
predict()
prediction
View(prediction)
prediction <- h2o.predict(model, real_test)
prediction
print(prediction)
prediction <- as.data.frame(h2o.predict(gbm_hbw, test))
prediction <- as.data.frame(h2o.predict(model, real_test))
prediction
h2o.shudown()
h2o.shutdown()
h2o.confusionMatrix(model, real_test)
h2o.init()
gbm <- h2o.getModel(path = "models/initial_train")
gbm <- h2o.getModel("models/initial_train")
gbm <- h2o.loadModel(path = "models/initial_train")
data_2018 <- read_csv("data/working/2018-2018-pace-working.csv", comment="#") %>%
mutate(`College Name` = as.factor(`College Name`)) %>%
mutate(FG.adj = FG/`College Pace`) %>%
mutate(FGA.adj = FGA/`College Pace`) %>%
mutate(`3P.adj` = `3P`/`College Pace`) %>%
mutate(`3Pa.adj` = `3Pa`/`College Pace`) %>%
mutate(FT.adj = FT/`College Pace`) %>%
mutate(FTA.adj = FTA/`College Pace`) %>%
mutate(ORB.adj = ORB/`College Pace`) %>%
mutate(TRB.adj = TRB/`College Pace`) %>%
mutate(AST.adj = AST/`College Pace`) %>%
mutate(STL.adj = STL/`College Pace`) %>%
mutate(BLK.adj = BLK/`College Pace`) %>%
mutate(TOV.adj = TOV/`College Pace`) %>%
mutate(PF.adj = PF/`College Pace`) %>%
mutate(PTS.adj = PTS/`College Pace`)
real_test <- as.h2o(data_2018)
prediction <- as.data.frame(h2o.predict(model, real_test))
prediction
h2o.confusionMatrix(model, real_test)
gbm@model$training_metrics@metrics$max_criteria_and_metric_scores
h2o.confusionMatrix(model, metrics = "f2")
h2o.confusionMatrix(model, metrics = "f1")
h2o.confusionMatrix(model, metrics = "f2")
h2o.confusionMatrix(h2o.predict(model, real_test), metrics = "f2")
2
h2o.confusionMatrix(model, metrics = "f2")
